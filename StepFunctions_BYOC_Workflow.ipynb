{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StepFunctions Data Science SDK for BYO Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate how to use the StepFunction Data Science SDK to do an end-to-end data science workflow where you bring your own code and deploy a model on SageMaker.\n",
    "\n",
    "The Steps are as follows:\n",
    "\n",
    "\n",
    "1/ Create a Lambda function which launches a CodeBuild job that launches the creation of your Docker container. (Steps for this are included separately)\n",
    "\n",
    "2/ Launch the Lambda function as a Step Functions workflow. \n",
    "\n",
    "3/ Once the Docker container is built, launch a SageMaker training job using SF DS SDK.\n",
    "\n",
    "4/ Use the DS SDK to deploy the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IAM Roles and Permissions\n",
    "\n",
    "Before running the code, ensure that your Amazon SageMaker notebook IAM role can call the AWS StepFunctions SDK, and vice-versa.\n",
    "\n",
    "To do this, follow the steps in the Setup Section of this notebook upto the section \"Configure Execution Roles\": https://github.com/awslabs/amazon-sagemaker-examples/blob/master/step-functions-data-science-sdk/machine_learning_workflow_abalone/machine_learning_workflow_abalone.ipynb\n",
    "\n",
    "\n",
    "If you have already completed this, then ignore this section and move on to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and upload the training data to S3\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket() # or feel free to replace with a bucket of your choosing\n",
    "WORK_DIRECTORY = 'PennFudanPed'\n",
    "key = 'BYO-Mask-RCNN'\n",
    "prefix = f'{key}/{WORK_DIRECTORY}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install StepFunctions SDK\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade stepfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stepfunctions\n",
    "import logging\n",
    "from stepfunctions.steps import (LambdaStep, Retry, Catch, Fail, Chain, TrainingStep, ModelStep, EndpointConfigStep, EndpointStep)\n",
    "from stepfunctions.workflow import Workflow\n",
    "from stepfunctions.template.pipeline import TrainingPipeline\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "# paste the StepFunctionsWorkflowExecutionRole ARN from above in \"<>\"\n",
    "workflow_execution_role = \"arn:aws:iam::520691797693:role/sentiment-analysis-pipeline-IamRole-493WVV3SG410\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the training dataset to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = sess.upload_data(WORK_DIRECTORY, bucket=bucket, key_prefix=prefix)\n",
    "print(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Estimator and StepFunctions Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "# Note that this image name will work below provided you have made the changes to the Environment variables in the Lambda\n",
    "#defintion as suggested in the workshop Readme. If not please make those first.\n",
    "\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/sm-container-maskrcnn:torch'.format(account, region) \n",
    "\n",
    "maskrcnn = sagemaker.estimator.Estimator(image,\n",
    "                       role, 1, 'ml.p2.xlarge', #feel free to modify with your own. A cost estimate is provided in Readme.\n",
    "                       output_path=\"s3://{}/{}/output\".format(sess.default_bucket(), key),\n",
    "                       sagemaker_session=sess)\n",
    "\n",
    "maskrcnn.set_hyperparameters(num_epochs = 1,\n",
    "                              num_classes = 2)\n",
    "\n",
    "#maskrcnn.fit(os.path.dirname(data_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create StepFunction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state = LambdaStep(\n",
    "    state_id=\"Calls CodeBuild to Build Container\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"lambda-build-docker\", #replace with the name of the Lambda function you created\n",
    "        \"Payload\": {  \n",
    "           \"input\": \"HelloWorld\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "lambda_state.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=15,\n",
    "    max_attempts=2,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_state.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"LambdaTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_input = ExecutionInput(schema={\n",
    "    'JobName': str, \n",
    "    'ModelName': str,\n",
    "    'EndpointName': str\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = TrainingStep(\n",
    "    'Train Step', \n",
    "    estimator=maskrcnn,\n",
    "#    role=workflow_execution_role,\n",
    "    data=os.path.dirname(data_location),\n",
    "    job_name=execution_input['JobName']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = ModelStep(\n",
    "    'Save model',\n",
    "    model=train_step.get_expected_model(),\n",
    "    model_name=execution_input['ModelName'] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step = EndpointConfigStep(\n",
    "    \"Create Endpoint Config\",\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    model_name=execution_input['ModelName'],\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_step = EndpointStep(\n",
    "    \"Create Endpoint\",\n",
    "    endpoint_name=execution_input['EndpointName'],\n",
    "    endpoint_config_name=execution_input['ModelName']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_definition = Chain([\n",
    "    lambda_state,\n",
    "    train_step,\n",
    "    model_step,\n",
    "    endpoint_config_step,\n",
    "    endpoint_step\n",
    "])\n",
    "\n",
    "# Next, we define the workflow\n",
    "workflow = Workflow(\n",
    "    name=\"MyWorkflow-BYOC-MaskRCNN-{}\".format(uuid.uuid1().hex),\n",
    "    definition=workflow_definition,\n",
    "    role=workflow_execution_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(workflow.definition.to_json(pretty=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.render_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = workflow.execute(\n",
    "    inputs={\n",
    "        'JobName': 'BYOC-Mask-RCNN-{}'.format(uuid.uuid1().hex), # Each Sagemaker Job requires a unique name\n",
    "        'ModelName': 'BYOC-Mask-RCNN-{}'.format(uuid.uuid1().hex), # Each Model requires a unique name,\n",
    "        'EndpointName': 'BYOC-Mask-RCNN-{}'.format(uuid.uuid1().hex) # Each Endpoint requires a unique name,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the progress of your workflow here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, display_html\n",
    "while True:\n",
    "    display_html(execution.render_progress())\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "\n",
    "Once your model is deployed, you can run inferences using this endpoint by using the SageMaker RealTimePredictor API. Please refer to the existing SageMaker documentation for how to do this.\n",
    "\n",
    "Also to ensure you don't rack up costs, make sure you delete the endpoint once you are done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
